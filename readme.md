# Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models

## Requirements provided in requirements.txt
```
python>=3.8
transformers>=4.22.2
numpy
pytorch>=1.10.0
scikit-learn
```

## Setup

1. Download dataset
   Download movieLens-1M dataset to folder `data/ml-1m/raw_data/`
2. Preprocessing: in folder `preprocess`
   1. run `python preprocess_ml-1m.py`.
   2. run `generate_data_and_prompt.py` to generate data for CTR  task, as well as prompt for LLM.
   
3. Knowledge generation

   Our knowledge generation version using Llama-2-7b-chat-hf is provided under `knowledge_generation/lm_generation.py`, but ultimately we used the knowledge from https://github.com/YunjiaXi/Open-World-Knowledge-Augmented-Recommendation.

   The following files should be downloaded from the above github linkand placed under `data/ml-1m/knowledge/`
   `data/ml-1m/item.klg`: factual knowledge about movies in MovieLens-1M.
   `data/ml-1m/user.klg`: preference knowledge about users in MovieLens-1M. 
   `data/ml-1m/begin_index.json`: To avoid information leakage, the authors provide `begin_index.json`, where each user has an index n. They have sorted the interaction sequence of each user by time (in ascending order) and gave each interaction an index. Then the first n interactions are used for prompting LLM to get preference knowledge. The first n interaction will not appear in the test set to prevent information leakage.
   place the files under `data/ml-1m/knowledge/`
   
   Above files are all saved in `json`, with each key as the raw id of user/item in the original dataset and each value as the corresponding prompt and knowledge generated by LLM.
   
4. Knowledge encoding: in folder `knowledge_encoding`
   Run `python lm_encoding.py`

5. RS: in folder `RS`
   1. adjust the parameters in `run_ctr.py` for training or testing.
   2. Run `python run_ctr.py` for ctr task


```
