# use BERT/chatGLM to encode the knowledge generated by LLM
import os
import json

import torch
from tqdm import tqdm
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModel

from utils import load_json, save_json, get_paragraph_representation
device = 'cuda'


def load_data(path):
    """
    Load the data from the path.
    """
    res = []
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        for id, value in data.items():
            res.append([id, value['prompt'], value['ans']])
    return res


def get_history_text(data_path, cold_start=False):
    """
    Get the history text from the data path.
    """
    raw_data = load_data(data_path)
    idx_list, hist_text = [], []
    for piece in raw_data:
        idx, prompt, answer = piece
        pure_hist = prompt[::-1].split(';', 1)[-1][::-1]
        if not cold_start:
            hist_text.append(pure_hist + '. ' + answer)
        else:
            # Extract only the demographic info from the prompt
            demo_info = prompt.split('.')[0].strip().removeprefix('Given ').split(',')[0]
            hist_text.append(demo_info + ' with no viewing history')
        idx_list.append(idx)
    return idx_list, hist_text


def get_item_text(data_path, no_interaction_item_id):
    """
    Get item text from the data path.
    """
    raw_data = load_data(data_path)
    idx_list, text_list = [], []
    for piece in raw_data:
        idx, prompt, answer = piece
        text_list.append(answer)
        idx_list.append(idx)
    text_list.append("no interaction item")
    idx_list.append(no_interaction_item_id)
    return idx_list, text_list


def get_text_data_loader(data_path, batch_size, item2id):
    """
    Get the dataloader for the text data.
    """
    hist_idxes, history = get_history_text(os.path.join(data_path, 'user.klg'))
    print('chatgpt.hist 1', history[1], 'hist len', len(history))
    item_idxes, items = get_item_text(os.path.join(data_path, 'item.klg'), no_interaction_item_id=item2id['no_interaction_item'])
    print('chatgpt.item 1', items[1], 'item len', len(items))
    hist_idxes_cs, history_cs = get_history_text(os.path.join(data_path, 'user.klg'), cold_start=True)
    history_loader = DataLoader(history, batch_size, shuffle=False)
    history_cs_loader = DataLoader(history_cs, batch_size, shuffle=False)
    item_loader = DataLoader(items, batch_size, shuffle=False)
    return history_loader, hist_idxes, history_cs_loader, hist_idxes_cs, item_loader, item_idxes


def remap_item(item_idxes, item_vec):
    """
    Remap item/history indices to item/history vectors.
    """
    item_vec_map = {}
    for idx, vec in zip(item_idxes, item_vec):
        item_vec_map[idx] = vec
    return item_vec_map


def inference(model, tokenizer, dataloader, model_name, aggregate_type):
    """
    Encode the text using the model.
    """
    pred_list = []
    model.eval()
    with torch.no_grad():
        for x in tqdm(dataloader):
            torch.cuda.empty_cache()
            if model_name == 'chatglm' or model_name == 'chatglm2':
                x = tokenizer(x, padding=True, truncation=True, return_tensors="pt",
                              return_attention_mask=True).to(device)
                mask = x['attention_mask']
                x.pop('attention_mask')
                outputs = model.transformer(**x, output_hidden_states=True, return_dict=True)
                outputs.last_hidden_state = outputs.last_hidden_state.transpose(1, 0)
            else:
                x = tokenizer(x, padding=True, truncation=True, max_length=512, return_tensors="pt",
                              return_attention_mask=True).to(device)
                mask = x['attention_mask']
                outputs = model(**x, output_hidden_states=True, return_dict=True)
            pred = get_paragraph_representation(outputs, mask, aggregate_type)
            pred_list.extend(pred.tolist())
    return pred_list


def main(knowledge_path, data_path, model_name, batch_size, aggregate_type):
    """
    Main function to encode the knowledge and save the results.
    Load --> encode --> map --> save
    """
    # load datamaps
    datamaps = load_json(os.path.join(data_path, 'datamaps.json'))
    # get dataloader
    hist_loader, hist_idxes, hist_cs_loader, hist_idxes_cs, item_loader, item_idxes = get_text_data_loader(knowledge_path, batch_size, item2id=datamaps['item2id'])

    if model_name == 'chatglm':
        checkpoint = '../../llm/chatglm-6b' if os.path.exists('../../llm/chatglm-6b') else 'chatglm-6b'
    elif model_name == 'chatglm2':
        checkpoint = '../../llm/chatglm-v2' if os.path.exists('../../llm/chatglm-v2') else 'chatglm-v2'
    elif model_name == 'bert':
        checkpoint = '../../llm/bert-base-uncased' if os.path.exists('../../llm/chatglm-6b') else 'bert-base-uncased'
    else:
        raise NotImplementedError

    # load model
    torch.cuda.empty_cache()
    tokenizer = AutoTokenizer.from_pretrained(checkpoint,  trust_remote_code=True)
    model = AutoModel.from_pretrained(checkpoint,  trust_remote_code=True).half().cuda()

    # encode item and history   
    item_vec = inference(model, tokenizer, item_loader, model_name, aggregate_type)
    hist_vec = inference(model, tokenizer, hist_loader, model_name, aggregate_type)
    hist_cs_vec = inference(model, tokenizer, hist_cs_loader, model_name, aggregate_type)
    # remap item and history indexes
    item_vec_dict = remap_item(item_idxes, item_vec)
    hist_vec_dict = remap_item(hist_idxes, hist_vec)
    hist_cs_vec_dict = remap_item(hist_idxes_cs, hist_cs_vec)

    # save item and history
    save_json(item_vec_dict, os.path.join(data_path, '{}_{}_augment.item'.format(model_name, aggregate_type)))
    save_json(hist_vec_dict, os.path.join(data_path, '{}_{}_augment.hist'.format(model_name, aggregate_type)))
    save_json(hist_cs_vec_dict, os.path.join(data_path, '{}_{}_augment_cs.hist'.format(model_name, aggregate_type)))
    stat_path = os.path.join(data_path, 'stat.json')
    with open(stat_path, 'r') as f:
        stat = json.load(f)

    stat['dense_dim'] = 4096 if model_name == 'chatglm' or model_name == 'chatglm2' else 768
    with open(stat_path, 'w') as f:
        stat = json.dumps(stat)
        f.write(stat)


if __name__ == '__main__':
    DATA_DIR = '/path/to/data/'
    DATA_SET_NAME = 'ml-1m'
    KLG_PATH = os.path.join(DATA_DIR, DATA_SET_NAME, 'knowledge')
    DATA_PATH = os.path.join(DATA_DIR, DATA_SET_NAME, 'proc_data')
    # MODEL_NAME = 'chatglm'
    # MODEL_NAME = 'chatglm2'
    MODEL_NAME = 'bert'  # bert, chatglm, chatglm2
    AGGREGATE_TYPE = 'avg'  # last, avg, wavg, cls, ...
    BATCH_SIZE = 16 if MODEL_NAME == 'bert' else 2
    main(KLG_PATH, DATA_PATH, MODEL_NAME, BATCH_SIZE, AGGREGATE_TYPE)

